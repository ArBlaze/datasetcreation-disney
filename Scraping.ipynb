{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538a9fee-acd7-428e-96cf-948fecf458cd",
   "metadata": {},
   "source": [
    "## Disney Dataset Creation (w/ BeautifulSoup)\n",
    "\n",
    "Scrape & clean a list of Disney Wikipedia pages to create a dataset to further analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb93c6-266d-45f3-b285-c88fdc67a944",
   "metadata": {},
   "source": [
    "#### Import Neccessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f7ef2a2-3e39-4985-bc02-56e404bbc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b7c1d8-07e9-4bac-8834-9f8b45b5cfe9",
   "metadata": {},
   "source": [
    "#### Load the webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01fdedd3-f124-4abb-a573-b490caf248bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load webpage content\n",
    "r = requests.get(\"https://en.wikipedia.org/wiki/Toy_Story_3\")\n",
    "\n",
    "# Convert to a beautiful soup object\n",
    "soup = bs(r.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fe59ec-9c83-4c60-a3aa-c4fe7c2b12f4",
   "metadata": {},
   "source": [
    "#### Grab the info box table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4591f650-1b84-4cc6-96ec-799a16286ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "info_box = soup.find(class_=\"infobox vevent\")\n",
    "info_rows = info_box.find_all(\"tr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5219572e-4da8-4907-9b36-052a7392c037",
   "metadata": {},
   "source": [
    "#### Write some functions to make this a lot more neater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7f8b491-8c06-40a8-80f5-4f8d0d4111e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-c8b4db6055ff>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-c8b4db6055ff>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    return rows.find(class_=\"infobox-data\").get_text(\" \", strip=True).replace(\"\\n\", \" \") +\u001b[0m\n\u001b[1;37m                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_title(rows):\n",
    "    return rows.get_text()\n",
    "\n",
    "\n",
    "def get_role(rows):\n",
    "    return rows.find(\"th\").get_text(\" \", strip=True)\n",
    "\n",
    "\n",
    "def get_people(rows):\n",
    "    return rows.find(class_=\"infobox-data\").get_text(\" \", strip=True).replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").replace(\"[1]\", \"\").strip()\n",
    "\n",
    "\n",
    "def if_list(row):\n",
    "    return [ls.get_text(\" \", strip=True).replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").replace(\"[1]\", \"\").strip() for ls in row.find_all(\"li\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc62f3-e32b-4f7b-ac38-a45239f5ea30",
   "metadata": {},
   "source": [
    "#### Store the table in movie_info dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55a3169-3f5b-4efc-af17-4b743fcf899c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movie_info = {}\n",
    "\n",
    "for index, row in enumerate(info_rows):\n",
    "    if index == 0:\n",
    "        movie_info['title'] = row.get_text()\n",
    "    elif index == 1:\n",
    "        continue\n",
    "    elif row.find(\"li\"):\n",
    "        movie_info[get_role(row)] = if_list(row)\n",
    "    else:\n",
    "        movie_info[get_role(row)] = get_people(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ae2d88-fd88-47f7-936a-77f6d1a10838",
   "metadata": {},
   "source": [
    "## Walt Disney Pictures Films Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2acb0-00d9-418e-9877-a8f124f59658",
   "metadata": {},
   "source": [
    "We'll now scrape the wikipedia page here: https://en.wikipedia.org/wiki/List_of_Walt_Disney_Pictures_films\n",
    "\n",
    "Specifically, we will get information of all the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "817ffe2c-afdf-4927-8149-f17e4935c20b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load webpage content\n",
    "d = requests.get(\"https://en.wikipedia.org/wiki/List_of_Walt_Disney_\" +\n",
    "                 \"Pictures_films\")\n",
    "\n",
    "# Create beautifulsoup object\n",
    "soupe = bs(d.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384f00e-a22e-4aaa-888a-4407f2eac0e9",
   "metadata": {},
   "source": [
    "#### Grab all information from the tableboxes, and then sort them in a neat way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a63e78-db71-42b6-ba9c-b0b346abfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_1 = soupe.find_all(class_=\"wikitable sortable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77beb391-d300-4bfa-a708-49cde23f04ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movie_dict = {}\n",
    "movie_list = []\n",
    "movie_title = \"\"\n",
    "for tables in table_1:\n",
    "    for index, tables_2 in enumerate(tables.find_all(\"tr\")):\n",
    "        if index == 0:\n",
    "            continue\n",
    "        for index, tables_3 in enumerate(tables_2.find_all(\"td\")):\n",
    "            movie_list.append(tables_3.get_text().replace(\n",
    "                \"\\n\", \" \").replace(\"\\xa0\", \" \").strip())\n",
    "        if movie_list == []:\n",
    "            continue\n",
    "        else:\n",
    "            movie_dict[movie_list.pop(1)] = movie_list\n",
    "            movie_list = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899995e4-295f-431e-950e-c9f21cff10f8",
   "metadata": {},
   "source": [
    "#### Now we're going to try something different.\n",
    "\n",
    "This time we will get all the names and links of the movies. Then, by accessing the links, we'll grab similar information from the task, similar to the elements we obtained from the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3006bbe9-e2a8-4548-a02d-a486172edd05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Grab html links links and names\n",
    "\n",
    "names = []\n",
    "links = []\n",
    "names_links = {}\n",
    "\n",
    "for table in table_1:\n",
    "    for table_2 in table.find_all(\"tr\"):\n",
    "        for table_3 in table_2.find_all(\"i\"):\n",
    "            for table_4 in table_3.find_all(\"a\", href=True):\n",
    "                names.append(table_4.get_text())\n",
    "                links.append('https://en.wikipedia.org/' + table_4['href'])\n",
    "                names_links[table_4.get_text(\n",
    "                )] = \"https://en.wikipedia.org/\" + table_4['href']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ddc65-cfdd-4c3d-84b4-1a31462650e6",
   "metadata": {},
   "source": [
    "Taking the earlier functions, and updating them to be a lot more universal for the task we're working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5fbcf0e-858a-457a-87dc-571806a331e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Requests(url):\n",
    "    # Load webpage content\n",
    "    b = requests.get(url)\n",
    "    # Convert to a beautiful soup object\n",
    "    soupa = bs(b.content, 'html.parser')\n",
    "    return soupa\n",
    "\n",
    "\n",
    "def get_info_box(soup):\n",
    "\n",
    "    info_box = soup.find(class_=\"infobox vevent\")\n",
    "    info_rows = info_box.find_all(\"tr\")\n",
    "\n",
    "    for row in info_rows:\n",
    "        if row.find(\"sup\"):\n",
    "            row.find(\"sup\").decompose()\n",
    "        elif row.find(class_=\"bday dtstart published updated\"):\n",
    "            row.find(class_=\"bday dtstart published updated\").decompose()\n",
    "        elif row.find(\"span\"):\n",
    "            row.find(\"span\").decompose()\n",
    "    return info_rows\n",
    "\n",
    "\n",
    "def get_title(rows):\n",
    "    return rows.find(class_=\"infobox-above summary\").get_text()\n",
    "\n",
    "\n",
    "def get_role(rows):\n",
    "    return rows.find(class_=\"infobox-label\").get_text(\" \", strip=True)\n",
    "\n",
    "\n",
    "def get_people(rows):\n",
    "    return rows.find(class_=\"infobox-data\").get_text(\" \", strip=True).replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").strip()\n",
    "\n",
    "\n",
    "def to_number(rows):\n",
    "\n",
    "    # This regex characters from a string and leaves only numerical values\n",
    "    return re.sub('\\D', '', rows.find(class_=\"infobox-data\").get_text())\n",
    "\n",
    "\n",
    "def break_sep(rows):\n",
    "    br = rows.find(class_=\"infobox-data\").get_text(separator=\",\", strip=True).split(\",\")\n",
    "    if len(br) == 1:\n",
    "        return br[0]\n",
    "    else:\n",
    "        return br\n",
    "\n",
    "\n",
    "def if_list(rows):\n",
    "    lst = [ls.get_text(\" \", strip=True).replace(\"\\n\", \" \").replace(\"\\xa0\", \" \").strip() for ls in rows.find_all(\"li\")]\n",
    "    if len(lst) == 1:\n",
    "        return lst[0]\n",
    "    else:\n",
    "        return lst\n",
    "\n",
    "\n",
    "def magic(enfo):\n",
    "\n",
    "    movie_info = {}\n",
    "    numerals = ['Budget', 'Box Office', 'Box office']\n",
    "    for row in enfo:\n",
    "        if row.find(class_=\"infobox-above summary\"):\n",
    "            movie_info[\"title\"] = get_title(row)\n",
    "        elif row.find(class_=\"infobox-label\"):\n",
    "            if row.find_all(\"li\"):\n",
    "                movie_info[get_role(row)] = if_list(row)\n",
    "            elif row.find_all(\"br\"):\n",
    "                movie_info[get_role(row)] = break_sep(row)\n",
    "            elif get_role(row) in numerals:\n",
    "                movie_info[get_role(row) + ' (in millions)'] = to_number(row)\n",
    "            elif get_role(row) == 'Running time':\n",
    "                movie_info[get_role(row) + ' (in hours)'] = to_number(row)\n",
    "            else:\n",
    "                movie_info[get_role(row)] = get_people(row)\n",
    "        else:\n",
    "            continue\n",
    "    return movie_info\n",
    "\n",
    "\n",
    "def Master(links):\n",
    "    master_list = []\n",
    "\n",
    "    for ln in links:\n",
    "        try:\n",
    "            req = Requests(ln)\n",
    "            info = get_info_box(req)\n",
    "            master_list.append(magic(info))\n",
    "        except Exception as e:\n",
    "            print(ln)\n",
    "            print(e)\n",
    "    return (master_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f04b48-b6a2-4f76-a968-b66119dd189d",
   "metadata": {},
   "source": [
    "Run links through the Master() function to obtain information from each Wikipedia article's information box. If the information box does not exist, it will not output. The error and the link that was unable to be scraped will be output just so we can ensure these errors are not the fault of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3941f5e2-e0b9-4102-ae7b-75f64a414953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org//wiki/True-Life_Adventures\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "https://en.wikipedia.org//wiki/Giannis_Antetokounmpo\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "https://en.wikipedia.org//wiki/The_Twilight_Zone_Tower_of_Terror#Film_adaptation\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "https://en.wikipedia.org//wiki/Chris_Paul\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "https://en.wikipedia.org//wiki/Jim_Henson#Legacy\n",
      "'NoneType' object has no attribute 'find_all'\n",
      "https://en.wikipedia.org//wiki/FC_Barcelona\n",
      "'NoneType' object has no attribute 'find_all'\n"
     ]
    }
   ],
   "source": [
    "master = Master(links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbaaf00-96e3-4751-ad05-d8d0769a8ab9",
   "metadata": {},
   "source": [
    "### Save/Load Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7cb688c-52f9-440a-8a69-2249e8a975c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def save_data(title, data):\n",
    "    with open(title, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b265143c-f65e-4ceb-b04e-cd58ec2c0ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def load_title(title):\n",
    "    with open(title, encoding='utf-8') as f:\n",
    "        json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d40af3bb-5637-4f01-ae10-ecaeb0d857da",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"./datasets/disney-data.json\", master)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
